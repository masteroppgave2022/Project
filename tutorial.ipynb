{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Incompatible GDAL 3.3.2 found on system. Internal GDAL 3.0.0 from distribution will be used.\n",
      "INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Internal GDAL 3.0.0 set to be used by SNAP.\n",
      "INFO: org.esa.snap.core.gpf.operators.tooladapter.ToolAdapterIO: Initializing external tool adapters\n",
      "INFO: org.esa.snap.core.util.EngineVersionCheckActivator: Please check regularly for new updates for the best SNAP experience.\n",
      "INFO: org.esa.s2tbx.dataio.gdal.GDALVersion: Internal GDAL 3.0.0 set to be used by SNAP.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors \n",
    "import os\n",
    "import snappy\n",
    "from snappy import Product\n",
    "from snappy import ProductIO \n",
    "from snappy import ProductUtils \n",
    "from snappy import WKTReader \n",
    "from snappy import HashMap\n",
    "from snappy import GPF\n",
    "# For shapefiles\n",
    "import shapefile \n",
    "import pygeoif\n",
    "import jpy\n",
    "\n",
    "path_to_sentinel_data = \"S1A_IW_GRDH_1SDV_20180415T163146_20180415T163211_021480_025003_8E79.SAFE/manifest.safe\"\n",
    "product = ProductIO.readProduct(path_to_sentinel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 25220 px\n",
      "Height: 16774 px\n",
      "Name: S1A_IW_GRDH_1SDV_20180415T163146_20180415T163211_021480_025003_8E79\n",
      "Band names: Amplitude_VH, Intensity_VH, Amplitude_VV, Intensity_VV\n"
     ]
    }
   ],
   "source": [
    "width = product.getSceneRasterWidth() \n",
    "print(\"Width: {} px\".format(width)) \n",
    "height = product.getSceneRasterHeight() \n",
    "print(\"Height: {} px\".format(height)) \n",
    "name = product.getName()\n",
    "print(\"Name: {}\".format(name))\n",
    "band_names = product.getBandNames()\n",
    "print(\"Band names: {}\".format(\", \".join(band_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show product inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBand(product, band, vmin, vmax):\n",
    "\n",
    "    band = product.getBand(band) \n",
    "    w = band.getRasterWidth()\n",
    "    h = band.getRasterHeight() \n",
    "    print(w, h)\n",
    "\n",
    "    band_data = np.zeros(w * h, np.float32) \n",
    "    band.readPixels(0, 0, w, h, band_data)\n",
    "\n",
    "    band_data.shape = h, w\n",
    "\n",
    "    width = 12\n",
    "    height = 12\n",
    "    plt.figure(figsize=(width, height))\n",
    "    imgplot = plt.imshow(band_data, cmap=plt.cm.binary, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    return imgplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pre-processing \n",
    "### Orbit file application\n",
    "Before any SAR pre-processing steps occur, the product subset should be properly orthorectified \n",
    "to improve accuracy. To properly orthorectify the image, the orbit file is applied using the \n",
    "Apply-Orbit-File GPF module. SNAP is able to generate and apply a high accuracy satellite \n",
    "orbit file to a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: org.hsqldb.persist.Logger: dataFileCache open start\n",
      "INFO: org.esa.snap.engine_utilities.download.DownloadableContentImpl: http retrieving http://step.esa.int/auxdata/orbits/Sentinel-1/POEORB/S1A/2018/04/S1A_OPER_AUX_POEORB_OPOD_20180505T120640_V20180414T225942_20180416T005942.EOF.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% done.\n"
     ]
    }
   ],
   "source": [
    "parameters = HashMap() \n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "\n",
    "parameters.put('Apply-Orbit-File', True)\n",
    "parameters.put('orbitType', 'Sentinel Precise (Auto Download)') \n",
    "parameters.put('polyDegree', '3') \n",
    "parameters.put('continueOnFail', 'false')\n",
    "\n",
    "apply_orbit_file = GPF.createProduct('Apply-Orbit-File', parameters, product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = shapefile.Reader(\"island_boundary/island_boundary2.shp\")\n",
    "g=[]\n",
    "for s in r.shapes(): \n",
    "    g.append(pygeoif.geometry.as_shape(s))\n",
    "\n",
    "m = pygeoif.MultiPoint(g)\n",
    "\n",
    "wkt = str(m.wkt).replace(\"MULTIPOINT\", \"POLYGON(\") + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import snappy\n",
    "from sentinelsat import SentinelAPI, geojson_to_wkt, read_geojson\n",
    "from snappy import ProductIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: S1A_IW_GRDH_1SDV_20190309T171105_20190309T171130_026264_02EF4D_0EDD | 1.58 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190309T171130_20190309T171155_026264_02EF4D_1310 | 1.58 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190309T171155_20190309T171220_026264_02EF4D_8325 | 1.58 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190308T171920_20190308T171945_015266_01C90B_0C57 | 1.66 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190308T171855_20190308T171920_015266_01C90B_D6BE | 1.66 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190307T163758_20190307T163827_015251_01C895_F18F | 1.83 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190307T163827_20190307T163852_015251_01C895_DE81 | 1.58 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190306T164658_20190306T164723_026220_02ED9E_2360 | 1.66 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190306T164633_20190306T164658_026220_02ED9E_33AD | 1.66 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190306T164723_20190306T164748_026220_02ED9E_E9BC | 1.66 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190305T165356_20190305T165421_015222_01C7A4_7AFB | 1.67 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190305T165421_20190305T165446_015222_01C7A4_0A7C | 1.67 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190305T165446_20190305T165511_015222_01C7A4_BEEE | 1.67 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190304T170246_20190304T170311_026191_02EC98_05A7 | 1.65 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190304T170311_20190304T170336_026191_02EC98_4076 | 1.65 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190304T170336_20190304T170401_026191_02EC98_E4E9 | 1.65 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190303T171110_20190303T171135_015193_01C6B2_62F6 | 1.63 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190303T171020_20190303T171045_015193_01C6B2_5630 | 1.63 GB\n",
      "title: S1B_IW_GRDH_1SDV_20190303T171045_20190303T171110_015193_01C6B2_41D4 | 1.63 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190302T171916_20190302T171941_026162_02EB87_9945 | 1.57 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190302T171941_20190302T172006_026162_02EB87_A30A | 1.57 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190302T172006_20190302T172031_026162_02EB87_D7AF | 1.58 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190301T163856_20190301T163921_026147_02EAF4_B793 | 1.66 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190301T163831_20190301T163856_026147_02EAF4_B1F1 | 1.66 GB\n",
      "title: S1A_IW_GRDH_1SDV_20190301T163921_20190301T163946_026147_02EAF4_4345 | 1.66 GB\n",
      "Total found 25 title of 41.0 GB\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m sar \u001b[38;5;241m=\u001b[39m sentinel1_download_preprocess(input_dir, start_date, end_date, query_style, footprint, lat, lon, \u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# proceed to download by setting 'True', default is 'False'\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[43msar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel1_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36msentinel1_download_preprocess.sentinel1_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(input_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m download_candidate\u001b[38;5;241m.\u001b[39mitems()]:\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNothing to download\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/sentinelsat/sentinel.py:715\u001b[0m, in \u001b[0;36mSentinelAPI.download_all\u001b[0;34m(self, products, directory_path, max_attempts, checksum, n_concurrent_dl, lta_retry_delay, fail_fast, nodefilter)\u001b[0m\n\u001b[1;32m    713\u001b[0m     downloader\u001b[38;5;241m.\u001b[39mlta_retry_delay \u001b[38;5;241m=\u001b[39m lta_retry_delay\n\u001b[1;32m    714\u001b[0m downloader\u001b[38;5;241m.\u001b[39mnode_filter \u001b[38;5;241m=\u001b[39m nodefilter\n\u001b[0;32m--> 715\u001b[0m statuses, exceptions, product_infos \u001b[38;5;241m=\u001b[39m \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Adapt results to the old download_all() API\u001b[39;00m\n\u001b[1;32m    718\u001b[0m downloaded_prods \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/sentinelsat/download.py:270\u001b[0m, in \u001b[0;36mDownloader.download_all\u001b[0;34m(self, products, directory)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ResultTuple({}, {}, {})\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWill download \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m products using \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m workers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(product_ids), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_concurrent_dl\n\u001b[1;32m    268\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m statuses, online_prods, offline_prods, product_infos, exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statuses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduct_ids\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Skip already downloaded files.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Although the download method also checks, we do not need to retrieve such\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# products from the LTA and use up our quota.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_existing_products(directory, offline_prods, product_infos, statuses, exceptions)\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/sentinelsat/download.py:390\u001b[0m, in \u001b[0;36mDownloader._init_statuses\u001b[0;34m(self, product_ids)\u001b[0m\n\u001b[1;32m    388\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Get online status and product info.\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproduct_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching archival status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pid, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/sentinelsat/sentinel.py:978\u001b[0m, in \u001b[0;36mSentinelAPI._tqdm\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m\"\"\"tqdm progressbar wrapper. May be overridden to customize progressbar behavior\"\"\"\u001b[39;00m\n\u001b[1;32m    977\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progressbars})\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/tqdm/notebook.py:242\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    241\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/localhome/studenter/renatask/.pyenv/versions/3.9.7/lib/python3.9/site-packages/tqdm/notebook.py:115\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIProgress not found. Please update jupyter and ipywidgets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See https://ipywidgets.readthedocs.io/en/stable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user_install.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    120\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: org.hsqldb.persist.Logger: Database closed\n"
     ]
    }
   ],
   "source": [
    "class sentinel1_download_preprocess():\n",
    "    def __init__(self, input_dir, date_1, date_2, query_style, footprint, lat=24.84, lon=90.43, download=False):\n",
    "        self.input_dir = input_dir\n",
    "        self.date_start = datetime.datetime.strptime(date_1, \"%d%b%Y\")\n",
    "        self.date_end = datetime.datetime.strptime(date_2, \"%d%b%Y\")\n",
    "        self.query_style = query_style\n",
    "        self.footprint = \"POLYGON ((5.712890625 61.990587736204105, 11.9091796875 61.990587736204105, 11.9091796875 64.64270382119372, 5.712890625 64.64270382119372, 5.712890625 61.990587736204105))\"#geojson_to_wkt(read_geojson(footprint))\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.download = download\n",
    "\n",
    "        # configurations\n",
    "        self.api = SentinelAPI('renateask', '071198Ra', 'https://scihub.copernicus.eu/dhus')\n",
    "        self.producttype = 'GRD'  # SLC, GRD, OCN\n",
    "        self.orbitdirection = 'ASCENDING'  # ASCENDING, DESCENDING\n",
    "        self.sensoroperationalmode = 'IW'  # SM, IW, EW, WV\n",
    "\n",
    "    def sentinel1_download(self):\n",
    "        global download_candidate\n",
    "        if self.query_style == 'coordinate':\n",
    "            download_candidate = self.api.query('POINT({0} {1})'.format(self.lon, self.lat),\n",
    "                                                date=(self.date_start, self.date_end),\n",
    "                                                producttype=self.producttype,\n",
    "                                                orbitdirection=self.orbitdirection,\n",
    "                                                sensoroperationalmode=self.sensoroperationalmode)\n",
    "        elif self.query_style == 'footprint':\n",
    "            download_candidate = self.api.query(self.footprint,\n",
    "                                                date=(self.date_start, self.date_end),\n",
    "                                                producttype=self.producttype,\n",
    "                                                orbitdirection=self.orbitdirection,\n",
    "                                                sensoroperationalmode=self.sensoroperationalmode)\n",
    "        else:\n",
    "            print(\"Define query attribute\")\n",
    "\n",
    "        title_found_sum = 0\n",
    "        for key, value in download_candidate.items():\n",
    "            for k, v in value.items():\n",
    "                if k == 'title':\n",
    "                    title_info = v\n",
    "                    title_found_sum += 1\n",
    "                elif k == 'size':\n",
    "                    print(\"title: \" + title_info + \" | \" + v)\n",
    "        print(\"Total found \" + str(title_found_sum) +\n",
    "              \" title of \" + str(self.api.get_products_size(download_candidate)) + \" GB\")\n",
    "\n",
    "        os.chdir(self.input_dir)\n",
    "        if self.download:\n",
    "            if glob.glob(input_dir + \"*.zip\") not in [value for value in download_candidate.items()]:\n",
    "                self.api.download_all(download_candidate)\n",
    "                print(\"Nothing to download\")\n",
    "        else:\n",
    "            print(\"Escaping download\")\n",
    "        # proceed processing after download is complete\n",
    "        self.sentinel1_preprocess()\n",
    "\n",
    "    def sentinel1_preprocess(self):\n",
    "        # Get snappy Operators\n",
    "        snappy.GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "        # HashMap Key-Value pairs\n",
    "        HashMap = snappy.jpy.get_type('java.util.HashMap')\n",
    "\n",
    "        for folder in glob.glob(self.input_dir + \"\\*\"):\n",
    "            gc.enable()\n",
    "            if folder.endswith(\".zip\"):\n",
    "                timestamp = folder.split(\"_\")[5]\n",
    "                sentinel_image = ProductIO.readProduct(folder)\n",
    "                if self.date_start <= datetime.datetime.strptime(timestamp[:8], \"%Y%m%d\") <= self.date_end:\n",
    "                    # add orbit file\n",
    "                    self.sentinel1_preprocess_orbit_file(timestamp, sentinel_image, HashMap)\n",
    "                    # remove border noise\n",
    "                    self.sentinel1_preprocess_border_noise(timestamp, HashMap)\n",
    "                    # remove thermal noise\n",
    "                    self.sentinel1_preprocess_thermal_noise_removal(timestamp, HashMap)\n",
    "                    # calibrate image to output to Sigma and dB\n",
    "                    self.sentinel1_preprocess_calibration(timestamp, HashMap)\n",
    "                    # TOPSAR Deburst for SLC images\n",
    "                    if self.producttype == 'SLC':\n",
    "                        self.sentinel1_preprocess_topsar_deburst_SLC(timestamp, HashMap)\n",
    "                    # multilook\n",
    "                    self.sentinel1_preprocess_multilook(timestamp, HashMap)\n",
    "                    # subset using a WKT of the study area\n",
    "                    self.sentinel1_preprocess_subset(timestamp, HashMap)\n",
    "                    # finally terrain correction, can use local data but went for the default \n",
    "                    self.sentinel1_preprocess_terrain_correction(timestamp, HashMap)\n",
    "                    # break # try this if you want to check the result one by one\n",
    "            \n",
    "    def sentinel1_preprocess_orbit_file(self, timestamp, sentinel_image, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        orb = self.input_dir + \"\\\\orb_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(orb + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            orbit_param = snappy.GPF.createProduct(\"Apply-Orbit-File\", parameters, sentinel_image)\n",
    "            ProductIO.writeProduct(orbit_param, orb, 'BEAM-DIMAP')  # BEAM-DIMAP, GeoTIFF-BigTiff\n",
    "            print(\"orbit file added: \" + orb +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + orb)\n",
    "\n",
    "    def sentinel1_preprocess_border_noise(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        border = self.input_dir + \"\\\\bordr_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(border + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            border_param = snappy.GPF.createProduct(\"Remove-GRD-Border-Noise\", parameters,\n",
    "                                                    ProductIO.readProduct(self.input_dir +\n",
    "                                                                          \"\\\\orb_\" + timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(border_param, border, 'BEAM-DIMAP')\n",
    "            print(\"border noise removed: \" + border +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + border)\n",
    "\n",
    "    def sentinel1_preprocess_thermal_noise_removal(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        thrm = self.input_dir + \"\\\\thrm_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(thrm + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            thrm_param = snappy.GPF.createProduct(\"ThermalNoiseRemoval\", parameters,\n",
    "                                                  ProductIO.readProduct(self.input_dir + \"\\\\bordr_\" +\n",
    "                                                                        timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(thrm_param, thrm, 'BEAM-DIMAP')\n",
    "            print(\"thermal noise removed: \" + thrm +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + thrm)\n",
    "\n",
    "    def sentinel1_preprocess_calibration(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        calib = self.input_dir + \"\\\\calib_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(calib + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            parameters.put('outputSigmaBand', True)\n",
    "            parameters.put('outputImageScaleInDb', False)\n",
    "            calib_param = snappy.GPF.createProduct(\"Calibration\", parameters,\n",
    "                                                   ProductIO.readProduct(self.input_dir + \"\\\\thrm_\" +\n",
    "                                                                         timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(calib_param, calib, 'BEAM-DIMAP')\n",
    "            print(\"calibration complete: \" + calib +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + calib)\n",
    "\n",
    "    def sentinel1_preprocess_topsar_deburst_SLC(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        deburst = self.input_dir + \"\\\\dburs_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(deburst):\n",
    "            parameters = HashMap()\n",
    "            parameters.put('outputSigmaBand', True)\n",
    "            parameters.put('outputImageScaleInDb', False)\n",
    "            deburst_param = snappy.GPF.createProduct(\"TOPSAR-Deburst\", parameters,\n",
    "                                                     ProductIO.readProduct(self.input_dir + \"\\\\calib_\" +\n",
    "                                                                           timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(deburst_param, deburst, 'BEAM-DIMAP')\n",
    "            print(\"deburst complete: \" + deburst +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + deburst)\n",
    "\n",
    "    def sentinel1_preprocess_multilook(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        multi = self.input_dir + \"\\\\multi_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(multi + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            parameters.put('outputSigmaBand', True)\n",
    "            parameters.put('outputImageScaleInDb', False)\n",
    "            multi_param = snappy.GPF.createProduct(\"Multilook\", parameters,\n",
    "                                                   ProductIO.readProduct(self.input_dir + \"\\\\calib_\" +\n",
    "                                                                         timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(multi_param, multi, 'BEAM-DIMAP')\n",
    "            print(\"multilook complete: \" + multi +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + multi)\n",
    "\n",
    "    def sentinel1_preprocess_subset(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        subset = self.input_dir + \"\\\\subset_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(subset + \".dim\"):\n",
    "            WKTReader = snappy.jpy.get_type('com.vividsolutions.jts.io.WKTReader')\n",
    "            \n",
    "            # converting shapefile to GEOJSON and WKT is easy with any free online tool\n",
    "            wkt = \"POLYGON((92.330290184197 20.5906091141114,89.1246637610338 21.6316051481971,\" \\\n",
    "                  \"89.0330319081811 21.7802436586492,88.0086282580443 24.6678836192818,88.0857830091018 \" \\\n",
    "                  \"25.9156771178278,88.1771488779853 26.1480664053835,88.3759125970998 26.5942658997298,\" \\\n",
    "                  \"88.3876586919721 26.6120432770312,88.4105534167129 26.6345128356038,89.6787084683935 \" \\\n",
    "                  \"26.2383305017275,92.348481691233 25.073636976939,92.4252199249342 25.0296592837972,\" \\\n",
    "                  \"92.487261172615 24.9472465376954,92.4967290851295 24.902213855393,92.6799861774377 \" \\\n",
    "                  \"21.2972058618174,92.6799346581579 21.2853347419811,92.330290184197 20.5906091141114))\"\n",
    "\n",
    "            geom = WKTReader().read(wkt)\n",
    "            parameters = HashMap()\n",
    "            parameters.put('geoRegion', geom)\n",
    "            subset_param = snappy.GPF.createProduct(\"Subset\", parameters,\n",
    "                                                    ProductIO.readProduct(self.input_dir + \"\\\\multi_\" +\n",
    "                                                                          timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(subset_param, subset, 'BEAM-DIMAP')\n",
    "            print(\"subset complete: \" + subset +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + subset)\n",
    "\n",
    "    def sentinel1_preprocess_terrain_correction(self, timestamp, HashMap):\n",
    "        start_time_processing = datetime.datetime.now()\n",
    "        terr = self.input_dir + \"\\\\terr_\" + timestamp\n",
    "\n",
    "        if not os.path.isfile(terr + \".dim\"):\n",
    "            parameters = HashMap()\n",
    "            # parameters.put('demResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "            # parameters.put('imgResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "            # parameters.put('pixelSpacingInMeter', 10.0)\n",
    "            terr_param = snappy.GPF.createProduct(\"Terrain-Correction\", parameters,\n",
    "                                                  ProductIO.readProduct(self.input_dir + \"\\\\subset_\" +\n",
    "                                                                        timestamp + \".dim\"))\n",
    "            ProductIO.writeProduct(terr_param, terr, 'BEAM-DIMAP')\n",
    "            print(\"terrain corrected: \" + terr +\n",
    "                  \" | took: \" + str(datetime.datetime.now() - start_time_processing).split('.', 2)[0])\n",
    "        else:\n",
    "            print(\"file exists - \" + terr)\n",
    "\n",
    "input_dir = \"/localhome/studenter/renatask/Project/testdata\"\n",
    "start_date = '01Mar2019'\n",
    "end_date = '10Mar2019'\n",
    "query_style = 'footprint' # 'footprint' to use a GEOJSON, 'coordinate' to use a lat-lon \n",
    "footprint = '/localhome/studenter/renatask/Project/testdata/map.geojson'\n",
    "lat = 26.23\n",
    "lon = 88.56\n",
    "\n",
    "sar = sentinel1_download_preprocess(input_dir, start_date, end_date, query_style, footprint, lat, lon, True) \n",
    "# proceed to download by setting 'True', default is 'False'\n",
    "sar.sentinel1_download()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48db88f1ebb28dacc274e5700a5b5df73d50efc7070be719e0a56543b127086a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
